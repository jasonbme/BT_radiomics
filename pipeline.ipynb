{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb035585-3b7a-4db3-8f98-0fc2be8221ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score,cohen_kappa_score, matthews_corrcoef\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "\n",
    "pd.options.mode.chained_assignment = None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f548fbf-1e52-43aa-a463-982d0daaacaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_data_split(raw_data, train_mapping, test_mapping):\n",
    "    train_data = pd.DataFrame(columns=raw_data.columns)\n",
    "    test_data = pd.DataFrame(columns=raw_data.columns)\n",
    "    \n",
    "    for case_id in raw_data.index:\n",
    "        case_general_id = case_id[0:20]\n",
    "        if case_general_id in train_mapping.values[:,1]:\n",
    "            train_data.loc[case_id]= raw_data.loc[case_id, :]\n",
    "        elif case_general_id in test_mapping.values[:,1]:\n",
    "            test_data.loc[case_id]= raw_data.loc[case_id, :]\n",
    "            \n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c4c1c0d-097b-4b89-ad94-e462d91c67a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features(dataset, number_of_features):\n",
    "    X = dataset.loc[:, dataset.columns != \"Grade\"]\n",
    "    Y = dataset.loc[:, \"Grade\"]\n",
    "    \n",
    "    standard_scaler = StandardScaler().fit(X)\n",
    "    X[X.columns] = standard_scaler.fit_transform(X[X.columns])\n",
    "    \n",
    "    select_k_best = SelectKBest(f_classif).fit(X, Y)\n",
    "    \n",
    "    scored_features = pd.DataFrame({'Feature':list(X.columns), 'Score':select_k_best.scores_})\n",
    "    scored_features.sort_values(by='Score', ascending=False, inplace=True)\n",
    "    scored_features.to_csv(\"output/scored_features.csv\")\n",
    "    \n",
    "    best_features = scored_features.nlargest(number_of_features,'Score')\n",
    "    best_features = best_features.loc[:, \"Feature\"]\n",
    "    # print(\"Selected features: \")\n",
    "    # print(best_features)\n",
    "    best_features.to_csv(\"output/best_features.csv\")\n",
    "\n",
    "    selected_features = X.loc[dataset.index, best_features]\n",
    "    selected_features[\"Grade\"] = Y\n",
    "    \n",
    "    return selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1cc0883-b3f3-4a85-a063-da2a1c4c469d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def equalize_dataset(dataset):\n",
    "    is_HGG =  dataset['Grade']=='HGG'\n",
    "    is_LGG =  dataset['Grade']=='LGG'\n",
    "    hggs = dataset[is_HGG]\n",
    "    lggs = dataset[is_LGG]\n",
    "    hggs = hggs.sample(n=len(lggs))\n",
    "    equalized_mapping = pd.concat([hggs, lggs])\n",
    "    return equalized_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a203da11-d69e-43be-b5c9-04ce726ad319",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_modalities(input_data):\n",
    "    \n",
    "    flair= pd.DataFrame(columns=input_data.columns)\n",
    "    t1c = pd.DataFrame(columns=input_data.columns)\n",
    "    t1= pd.DataFrame(columns=input_data.columns)\n",
    "    t2 = pd.DataFrame(columns=input_data.columns)\n",
    "    \n",
    "    for case_id in input_data.index:\n",
    "        if \"FLAIR\" in case_id:\n",
    "            flair.loc[case_id]= input_data.loc[case_id, :]\n",
    "        elif \"T1C\" in case_id:\n",
    "            t1c.loc[case_id]= input_data.loc[case_id, :]\n",
    "        elif \"T1\" in case_id:\n",
    "            t1.loc[case_id]= input_data.loc[case_id, :]\n",
    "        elif \"T2\" in case_id:\n",
    "            t2.loc[case_id]= input_data.loc[case_id, :]\n",
    "            \n",
    "            \n",
    "    labels = (flair.loc[:, 'Grade']).to_numpy()\n",
    "    \n",
    "    flair.drop('Grade',axis='columns', inplace=True)\n",
    "    t1c.drop('Grade',axis='columns', inplace=True)\n",
    "    t1.drop('Grade',axis='columns', inplace=True)\n",
    "    t2.drop('Grade',axis='columns', inplace=True)\n",
    "    \n",
    "    return flair, t1c, t1, t2, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "925f868a-8996-4ea2-ba02-a89e95c15919",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_test_dataset(dataset):\n",
    "    selected_features = pd.read_csv(\"output/best_features.csv\", index_col=0)\n",
    "    features_list = selected_features[\"Feature\"].to_list()\n",
    "    \n",
    "    X = dataset.loc[:, dataset.columns != \"Grade\"]\n",
    "    Y = dataset.loc[:, \"Grade\"]\n",
    "    \n",
    "    reduced_X = X.loc[:, features_list]\n",
    "    \n",
    "    standard_scaler = StandardScaler().fit(X)\n",
    "    reduced_X[reduced_X.columns] = standard_scaler.fit_transform(reduced_X[reduced_X.columns])\n",
    "    \n",
    "    reduced_dataset = reduced_X\n",
    "    reduced_dataset[\"Grade\"] = Y\n",
    "    \n",
    "    return reduced_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bf0e940-dcb3-411c-ac71-eaf89708f01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_majority(voting_arr):\n",
    "    lgg_occurences = voting_arr.count(\"LGG\")\n",
    "    hgg_occurences = voting_arr.count(\"HGG\")\n",
    "    if  hgg_occurences >= lgg_occurences:\n",
    "        return \"HGG\"\n",
    "    else:\n",
    "        return \"LGG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29f5c6c8-21ce-42e4-8132-b1662dce41c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_majority_proba(proba_arr):\n",
    "    \n",
    "    hgg_proba = 0\n",
    "    lgg_proba = 0\n",
    "    \n",
    "    for modality_proba in proba_arr:\n",
    "        hgg_proba += modality_proba[0]\n",
    "        lgg_proba += modality_proba[1]\n",
    "        \n",
    "    if  hgg_proba >= lgg_proba:\n",
    "        return \"HGG\"\n",
    "    else:\n",
    "        return \"LGG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e19c2dde-0426-4aab-ac34-ee295e4b5746",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_metrics(overall_acc, overall_kappa, overall_mcc): \n",
    "    general_accuracy = pd.DataFrame({'Accuracy': overall_acc})\n",
    "    general_kappa = pd.DataFrame({'Kappa': overall_kappa})\n",
    "    general_mcc = pd.DataFrame({'MCC': overall_mcc})\n",
    "\n",
    "    accuracy_mean = float(general_accuracy.mean())\n",
    "    accuracy_median = float(general_accuracy.median())\n",
    "    accuracy_Q1 = general_accuracy.Accuracy.quantile([0.25]).to_numpy()[0]\n",
    "    accuracy_Q3 = general_accuracy.Accuracy.quantile([0.75]).to_numpy()[0]\n",
    "    accuracy_IQR = accuracy_Q3 - accuracy_Q1\n",
    "\n",
    "    kappa_mean = float(general_kappa.mean())\n",
    "    kappa_median = float(general_kappa.median())\n",
    "    kappa_Q1 = general_kappa.Kappa.quantile([0.25]).to_numpy()[0]\n",
    "    kappa_Q3 = general_kappa.Kappa.quantile([0.75]).to_numpy()[0]\n",
    "    kappa_IQR = kappa_Q3 - kappa_Q1\n",
    "\n",
    "    mcc_mean = float(general_mcc.mean())\n",
    "    mcc_median = float(general_mcc.median())\n",
    "    mcc_Q1 = general_mcc.MCC.quantile([0.25]).to_numpy()[0]\n",
    "    mcc_Q3 = general_mcc.MCC.quantile([0.75]).to_numpy()[0]\n",
    "    mcc_IQR = mcc_Q3 - mcc_Q1\n",
    "\n",
    "    accuracy_list = [accuracy_mean, accuracy_median, accuracy_Q1, accuracy_Q3, accuracy_IQR]\n",
    "    kappa_list = [kappa_mean, kappa_median, kappa_Q1, kappa_Q3, kappa_IQR]\n",
    "    mcc_list = [mcc_mean, mcc_median, mcc_Q1, mcc_Q3, mcc_IQR]\n",
    "\n",
    "    rows_labels = [\"Mean\", \"Median\", \"Q1\", \"Q3\", \"IQR\"]\n",
    "    column_labels = [\"Accuracy\", \"Kappa\", \"MCC\"]\n",
    "    metrics = pd.DataFrame(index=rows_labels, columns=column_labels)\n",
    "\n",
    "    metrics[\"Accuracy\"] = accuracy_list\n",
    "    metrics[\"Kappa\"] = kappa_list\n",
    "    metrics[\"MCC\"] = mcc_list\n",
    "\n",
    "    rounded_metrics = metrics.round(3)\n",
    "    rounded_metrics.to_csv(\"metrics.csv\")\n",
    "    return rounded_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3585251d-6f33-4871-846d-3472b9f78d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "FOLD NUMBER:  1\n",
      "\n",
      "\n",
      "Train labels distribution: Counter({'HGG': 233, 'LGG': 61})\n",
      "Test labels distribution: Counter({'HGG': 59, 'LGG': 15})\n",
      "\n",
      "\n",
      "488\n",
      "135\n",
      "125\n",
      "117\n",
      "111\n",
      "135\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [125, 135]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/cy/4fv0jb6n2q30z5ddjjtghjqh0000gn/T/ipykernel_42201/2566589016.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mflair_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflair_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0mt1c_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt1c_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0mt1_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt1_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mt2_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt2_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/thesis/lib/python3.7/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sparse multilabel-indicator for y is not supported.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m         X, y = self._validate_data(\n\u001b[0;32m--> 328\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m         )\n\u001b[1;32m    330\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/thesis/lib/python3.7/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    574\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    577\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/thesis/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    971\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmulti_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_numeric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_numeric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    974\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/thesis/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    331\u001b[0m         raise ValueError(\n\u001b[1;32m    332\u001b[0m             \u001b[0;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m             \u001b[0;34m%\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m         )\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [125, 135]"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"output/features.csv\", index_col=0)\n",
    "name_mapping = pd.read_csv(\"mapping/name_mapping.csv\")\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "X = name_mapping.values[:,1]\n",
    "y = name_mapping.values[:,0]\n",
    "fold_iterator = 1\n",
    "\n",
    "overall_acc = []\n",
    "overall_kappa = []\n",
    "overall_mcc = []\n",
    "\n",
    "number_of_features = 44\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    print(\"\\n\")\n",
    "    print(\"FOLD NUMBER: \", fold_iterator)\n",
    "    print(\"\\n\")\n",
    "    fold_iterator += 1\n",
    "     \n",
    "    X_train, X_test = X[train_index], X[test_index] \n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    print(\"Train labels distribution:\", Counter(y_train))\n",
    "    print(\"Test labels distribution:\",Counter(y_test))\n",
    "    print(\"\\n\")\n",
    "\n",
    "    train_mapping = pd.DataFrame({'Grade': y_train, 'ID': X_train})    \n",
    "    test_mapping = pd.DataFrame({'Grade': y_test, 'ID': X_test})\n",
    "    \n",
    "    (train_dataset, test_dataset) = train_test_data_split(dataset, train_mapping, test_mapping)\n",
    "    \n",
    "    reduced_train_dataset = select_features(train_dataset, number_of_features)\n",
    "    \n",
    "    equalized_train_dataset = equalize_dataset(reduced_train_dataset)\n",
    "    \n",
    "    print(len(equalized_train_dataset))\n",
    "    \n",
    "    (flair_data, t1c_data, t1_data, t2_data, labels) = split_modalities(equalized_train_dataset)\n",
    "    \n",
    "    print(len(flair_data))\n",
    "    print(len(t1c_data))\n",
    "    print(len(t1_data))\n",
    "    print(len(t2_data))\n",
    "    print(len(labels))\n",
    "\n",
    "    \n",
    "    flair_classifier = RandomForestClassifier()\n",
    "    t1c_classifier = RandomForestClassifier()\n",
    "    t1_classifier = RandomForestClassifier()\n",
    "    t2_classifier = RandomForestClassifier()\n",
    "    \n",
    "    flair_classifier.fit(flair_data.to_numpy(), labels)\n",
    "    t1c_classifier.fit(t1c_data.to_numpy(), labels)\n",
    "    t1_classifier.fit(t1_data.to_numpy(), labels)\n",
    "    t2_classifier.fit(t2_data.to_numpy(), labels)\n",
    "    \n",
    "    print(\"Classifiers training finished\")\n",
    "\n",
    "    preprocessed_test_dataset = preprocess_test_dataset(test_dataset)\n",
    "    \n",
    "    (test_flair_data, test_t1c_data, test_t1_data, test_t2_data, test_labels) = split_modalities(preprocessed_test_dataset)\n",
    "    \n",
    "    number_of_test_cases = len(test_mapping)\n",
    "    \n",
    "    test_results = []\n",
    "\n",
    "    for i in range(number_of_test_cases):\n",
    "        probability_array = []\n",
    "\n",
    "        flair_result = flair_classifier.predict_proba(test_flair_data.iloc[i].to_numpy().reshape(1,-1))[0]\n",
    "        probability_array.append(flair_result)\n",
    "            \n",
    "        t1c_result = t1c_classifier.predict_proba(test_t1c_data.iloc[i].to_numpy().reshape(1,-1))[0]\n",
    "        probability_array.append(t1c_result)\n",
    "            \n",
    "        t1_result = t1_classifier.predict_proba(test_t1_data.iloc[i].to_numpy().reshape(1,-1))[0]\n",
    "        probability_array.append(t1_result)\n",
    "            \n",
    "        t2_result = t2_classifier.predict_proba(test_t2_data.iloc[i].to_numpy().reshape(1,-1))[0]\n",
    "        probability_array.append(t2_result)\n",
    "                \n",
    "        final_prediction = check_majority_proba(probability_array)\n",
    "\n",
    "        test_results.append(final_prediction)\n",
    "        \n",
    "    test_results = np.array(test_results)\n",
    "    \n",
    "    print(confusion_matrix(test_labels, test_results))\n",
    "    # print(classification_report(test_labels, test_results))\n",
    "    \n",
    "    acc = accuracy_score(test_labels, test_results)\n",
    "    kappa = cohen_kappa_score(test_labels, test_results)\n",
    "    mcc = matthews_corrcoef(test_labels, test_results)\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(\"Accuracy: {:.5f}\".format(acc))\n",
    "    print(\"Cohen's Kappa: {:.5f}\".format(kappa))\n",
    "    print(\"MCC: {:.5f}\".format(mcc))\n",
    "    \n",
    "    overall_acc.append(acc)\n",
    "    overall_kappa.append(kappa)\n",
    "    overall_mcc.append(mcc)  \n",
    "\n",
    "metrics = prepare_metrics(overall_acc, overall_kappa, overall_mcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec78881c-5114-441a-8f49-0e412cea4b68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Mean</th>\n",
       "      <td>0.717</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Median</th>\n",
       "      <td>0.781</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q1</th>\n",
       "      <td>0.608</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>-0.039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q3</th>\n",
       "      <td>0.797</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IQR</th>\n",
       "      <td>0.189</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.219</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Accuracy  Kappa    MCC\n",
       "Mean       0.717  0.070  0.084\n",
       "Median     0.781  0.000  0.000\n",
       "Q1         0.608 -0.039 -0.039\n",
       "Q3         0.797  0.163  0.180\n",
       "IQR        0.189  0.202  0.219"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85671eed-1e1d-4e73-9dba-5f858b671ba2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
